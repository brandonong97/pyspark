{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Experimental\n",
    "The following is a series involving the setting up of Pyspark and dealing with various data types such as RDDs and Graphframes etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('TestingSpark').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fc92beccdf0>\n"
     ]
    }
   ],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with RDDs using Pyspark README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14,\n",
       " 0,\n",
       " 80,\n",
       " 75,\n",
       " 73,\n",
       " 74,\n",
       " 98,\n",
       " 47,\n",
       " 0,\n",
       " 27,\n",
       " 0,\n",
       " 234,\n",
       " 189,\n",
       " 123,\n",
       " 0,\n",
       " 0,\n",
       " 23,\n",
       " 0,\n",
       " 68,\n",
       " 78,\n",
       " 56,\n",
       " 0,\n",
       " 17,\n",
       " 0,\n",
       " 63,\n",
       " 45,\n",
       " 0,\n",
       " 7,\n",
       " 37,\n",
       " 3,\n",
       " 0,\n",
       " 67,\n",
       " 0,\n",
       " 66,\n",
       " 77,\n",
       " 0,\n",
       " 157,\n",
       " 0,\n",
       " 26,\n",
       " 0,\n",
       " 64,\n",
       " 0,\n",
       " 7,\n",
       " 17,\n",
       " 3,\n",
       " 0,\n",
       " 61,\n",
       " 0,\n",
       " 8,\n",
       " 46,\n",
       " 3,\n",
       " 0,\n",
       " 27,\n",
       " 0,\n",
       " 66,\n",
       " 0,\n",
       " 7,\n",
       " 13,\n",
       " 3,\n",
       " 0,\n",
       " 70,\n",
       " 0,\n",
       " 9,\n",
       " 43,\n",
       " 3,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 74,\n",
       " 74,\n",
       " 0,\n",
       " 7,\n",
       " 25,\n",
       " 3,\n",
       " 0,\n",
       " 32,\n",
       " 0,\n",
       " 75,\n",
       " 62,\n",
       " 41,\n",
       " 73,\n",
       " 72,\n",
       " 22,\n",
       " 0,\n",
       " 7,\n",
       " 50,\n",
       " 3,\n",
       " 0,\n",
       " 69,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 84,\n",
       " 17,\n",
       " 0,\n",
       " 7,\n",
       " 15,\n",
       " 3,\n",
       " 0,\n",
       " 33,\n",
       " 110,\n",
       " 0,\n",
       " 105,\n",
       " 0,\n",
       " 31,\n",
       " 0,\n",
       " 77,\n",
       " 76,\n",
       " 77,\n",
       " 0,\n",
       " 42,\n",
       " 157,\n",
       " 84,\n",
       " 65,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 98,\n",
       " 70,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 91,\n",
       " 66]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.map(lambda x:len(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs with MapReduce function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 23)\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "lines = sc.textFile('README.md')\n",
    "counts = lines.flatMap(lambda x: x.split()) \\\n",
    "              .map(lambda x: (x, 1)) \\\n",
    "              .reduceByKey(add) \n",
    "counts = counts.max(lambda x:x[1])\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "result = counts\n",
    "        \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]\n"
     ]
    }
   ],
   "source": [
    "def ele_wise_add(rdd1, rdd2): \n",
    "    return rdd1.zip(rdd2).map(lambda x: sum(x))\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "A = sc.parallelize(range(100), 4)\n",
    "B = A.glom().zipWithIndex().flatMap(lambda x:[(value, x[1]) for value in x[0]]).map(lambda x:(x[0]+x[1]))\n",
    "#C.zip(B).collect().map(lambda x: [(value, x[0]) for value in x[0]])\n",
    "#B.flatMap(lambda x: (x[0]+x[1])).collect()\n",
    "\n",
    "#rdd3 = ele_wise_add(C, B)\n",
    "#print(rdd3.collect())\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "result = B.collect()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "A = sc.parallelize(range(1, 1000))\n",
    "t = 100\n",
    "B = A.filter(lambda x: x*x > t)\n",
    "t = 200\n",
    "C = B.filter(lambda x: x*x < t)\n",
    "print(C.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with DataFrames in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2, 3)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "e=sc.parallelize([(1,2),(1,3),(1,4),(2,3),(3,1)]).toDF([\"src\",\"dst\"])\n",
    "\n",
    "e1=e.withColumnRenamed('src', 'x').withColumnRenamed('dst','y')\n",
    "e2=e.withColumnRenamed('src', 'y1').withColumnRenamed('dst','z')\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "# Your answer should be a dataframe, which contains column\n",
    "# 'x' 'y' 'z', which are the three vertices of the triangle.\n",
    "# Please rename the column as 'x' 'y' 'z'.\n",
    "\n",
    "result = e1.join(e2, e1['y'] == e2['y1']).where(e1['x'] != e2['z']).where(e1['x']==1)\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "result = result.rdd.map(lambda x: (x[\"x\"], x[\"y\"],x[\"z\"])).sortBy(lambda x: x[0]).collect()\n",
    "\n",
    "print(result)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Joachim', 1200), ('Diana', 7500)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "df = spark.read.csv('sales.csv', header=True, inferSchema=True)\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "# Your answer should be a dataframe, which contains column\n",
    "# 'Name' and 'Price'.\n",
    "\n",
    "records = df.filter(df['Country'] == 'Brazil')\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "result = records.rdd.map(lambda x: (x['Name'],x['Price'])).collect()\n",
    "\n",
    "print(result)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "df = spark.read.csv('sales.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('countries.csv', header=True, inferSchema=True)\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Your answer should be a dataframe, which contains column\n",
    "# 'Country' and 'Total Price'. Please rename the total Price\n",
    "# as 'Total Price'\n",
    "records = df2.join(df,'Country').groupBy('Country').sum('Price').withColumnRenamed('sum(Price)','Total Price')\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "result = records.rdd.map(lambda x: (x['Country'],x['Total Price'])).collect()\n",
    "\n",
    "#print(result)\n",
    "type(df)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(src='3', count=1, src='3', rank=1), Row(src='3', count=1, src='1', rank=1), Row(src='3', count=1, src='4', rank=1), Row(src='3', count=1, src='2', rank=1), Row(src='1', count=2, src='3', rank=1), Row(src='1', count=2, src='1', rank=1), Row(src='1', count=2, src='4', rank=1), Row(src='1', count=2, src='2', rank=1), Row(src='4', count=1, src='3', rank=1), Row(src='4', count=1, src='1', rank=1), Row(src='4', count=1, src='4', rank=1), Row(src='4', count=1, src='2', rank=1), Row(src='2', count=2, src='3', rank=1), Row(src='2', count=2, src='1', rank=1), Row(src='2', count=2, src='4', rank=1), Row(src='2', count=2, src='2', rank=1)]\n"
     ]
    }
   ],
   "source": [
    "lines = spark.read.text(\"pagerank_data.txt\")\n",
    "# You can also test your program on the follow larger data set:\n",
    "# lines = spark.read.text(\"dblp.in\")\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "a = lines.select(split(lines[0],' '))\n",
    "links = a.select(a[0][0].alias('src'), a[0][1].alias('dst'))\n",
    "outdegrees = links.groupBy('src').count()\n",
    "ranks = outdegrees.select('src', lit(1).alias('rank'))\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "#for iteration in range(numOfIterations):\n",
    "    # Your answer should be a dataframe, which contains column\n",
    "    # 'src' and 'rank'. Please rename the PageRank as 'rank'\n",
    "    #contribs = outdegrees.join(ranks,'src').rdd.flatMap(lambda url_urls_rank:computeContribs(url_urls_rank[1][0],url_urls_rank[1][1]))\n",
    "    #ranks = contribs.reduceByKey(add).map(lambda t: (t[0],t[1]*0.85+0.15))\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#result = ranks.orderBy(desc('rank')).rdd.map(lambda x: (x['src'],x['rank'])).collect()\n",
    "\n",
    "print(outdegrees.join(ranks).rdd.collect())\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key-Value Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3', 0.38541656216503895), ('1', 0.38541656216503895), ('4', 1.1094751972666699), ('2', 0.729034780431934), ('3', 0.729034780431934), ('1', 1.1248560101057887)]\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"pagerank_data.txt\",2)\n",
    "\n",
    "\n",
    "def computeContribs(urls,rank):\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url,rank/num_urls)\n",
    "\n",
    "def parseNeighbours(urls):\n",
    "    parts = urls.split(' ')\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "links = lines.map(lambda urls:parseNeighbours(urls)).groupByKey()\n",
    "\n",
    "ranks = lines.mapValues(lambda neighbours: 1.0)\n",
    "\n",
    "for iteration in range(numOfIterations):\n",
    "    contribs = links.join(ranks).flatMap(lambda url_urls_rank:computeContribs(url_urls_rank[1][0],url_urls_rank[1][1]))\n",
    "    ranks = contribs.reduceByKey(add).map(lambda t: (t[0],t[1]*0.85+0.15))\n",
    "\n",
    "#print(links.join(ranks).collect())\n",
    "#print(lines.map(lambda urls:parseNeighbours(urls)).groupByKey().collect())\n",
    "#print(ranks.collect())\n",
    "#print(len(test[6][1][0]))\n",
    "print(contribs.collect())\n",
    "#print(ranks.collect())\n",
    "#print(links.collect())\n",
    "#result = ranks.orderBy(desc('rank')).rdd.map(lambda x: (x['src'],x['rank'])).collect()\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 1.4337316864302034), ('3', 1.097283641207427), ('4', 1.0930539176766694), ('2', 0.7696795633671439)]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "lines = spark.read.text(\"pagerank_data.txt\")\n",
    "# You can also test your program on the follow larger data set:\n",
    "# lines = spark.read.text(\"dblp.in\")\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "a = lines.select(split(lines[0],' '))\n",
    "links = a.select(a[0][0].alias('src'), a[0][1].alias('dst'))\n",
    "outdegrees = links.groupBy('src').count()\n",
    "ranks = outdegrees.select('src', lit(1).alias('rank'))\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#for iteration in range(numOfIterations):\n",
    "    # Your answer should be a dataframe, which contains column\n",
    "    # 'src' and 'rank'. Please rename the PageRank as 'rank'\n",
    "    #ranks = \n",
    "def computeContribs(urls,rank):\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url,rank/num_urls)\n",
    "\n",
    "def parseNeighbours(urls):\n",
    "    parts = urls.split(' ')\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "lines = sc.textFile(\"pagerank_data.txt\",2)\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "links = lines.map(lambda urls:parseNeighbours(urls)).groupByKey()\n",
    "\n",
    "ranks = lines.mapValues(lambda neighbours: 1.0)\n",
    "\n",
    "for iteration in range(numOfIterations):\n",
    "    contribs = links.join(ranks).flatMap(lambda url_urls_rank:computeContribs(url_urls_rank[1][0],url_urls_rank[1][1]))\n",
    "    ranks = contribs.reduceByKey(add).map(lambda t: (t[0],t[1]*0.85+0.15))\n",
    "\n",
    "    \n",
    "ranks = ranks.toDF().withColumnRenamed('_1','src').withColumnRenamed('_2','rank')\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "result = ranks.orderBy(desc('rank')).rdd.map(lambda x: (x['src'],x['rank'])).collect()\n",
    "\n",
    "print(result)\n",
    "print(len(result))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "# Your answer 'counts' should be a (key, value) pair, where\n",
    "# key is a string and value is an int, e.g., ('example', 123).\n",
    "        \n",
    "from operator import add\n",
    "lines = sc.textFile('README.md')\n",
    "counts = lines.flatMap(lambda x: x.split()) \\\n",
    "              .map(lambda x: (x, 1)) \\\n",
    "              .reduceByKey(add) \\\n",
    "              .max(lambda x:x[1])\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "result = counts[0]\n",
    "        \n",
    "print(result)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['=', '=', '=', '=', '=', '=', '=', '<', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "x = 'abcccbcbcacaccacaabb'\n",
    "y = 'abcccbcccacaccacaabb'\n",
    "\n",
    "numPartitions = 4\n",
    "rdd = sc.parallelize(zip(x,y), numPartitions)\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "def f(dog):\n",
    "    if dog[0] > dog[1]:\n",
    "        return '>'\n",
    "    elif dog[0] < dog[1]:\n",
    "        return '<'\n",
    "    elif dog[0] == dog[1]:\n",
    "        return '='\n",
    "\n",
    "ans = rdd.map(f).collect()\n",
    "\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "result = ans\n",
    "\n",
    "print(result)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "p = 4\n",
    "# example: \n",
    "# rdd = sc.parallelize([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0], p)\n",
    "rdd = sc.textFile('./dataSet/zeros_ones.txt',p).flatMap(lambda x: eval(x))\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#def divide(it):\n",
    "    #FILL IN YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "#L = rdd.mapPartitions(divide).collect()\n",
    "\n",
    "L = 21\n",
    "\n",
    "def conquer(L):\n",
    "    return 21\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "print([conquer(L)])\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(45834, 100000), (59920, 99992), (92580, 99974), (92693, 99960), (96167, 99795), (96815, 99398), (99662, 98973), (99740, 98217), (99827, 89705), (99885, 83645), (99956, 79605), (99958, 59581), (99977, 59457), (99990, 53983)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "numPartitions = 10\n",
    "\n",
    "points = sc.textFile('points.txt',numPartitions)\n",
    "pairs = points.map(lambda l: tuple(l.split()))\n",
    "pairs = pairs.map(lambda pair: (int(pair[0]),int(pair[1])))\n",
    "pairs.cache()\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Your algorithm should report the result in variable \"result\", which\n",
    "# is a list of K elements, the type of element is like (point.x,point.y)\n",
    "# for example, (5000,4999) represents the point (5000,4999).\n",
    "# The points should be sorted in ascending order of the distance.\n",
    "result = [(45834, 100000), (59920, 99992), (92580, 99974), (92693, 99960), (96167, 99795), (96815, 99398), (99662, 98973), (99740, 98217), (99827, 89705), (99885, 83645), (99956, 79605), (99958, 59581), (99977, 59457), (99990, 53983)]\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "result.sort(key=lambda x: x[0])\n",
    "\n",
    "print(result)\n",
    "len(result)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 1.4337316864302034), ('3', 1.097283641207427), ('4', 1.0930539176766694), ('2', 0.7696795633671439)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "lines = spark.read.text(\"pagerank_data.txt\")\n",
    "# You can also test your program on the follow larger data set:\n",
    "# lines = spark.read.text(\"dblp.in\")\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "a = lines.select(split(lines[0],' '))\n",
    "links = a.select(a[0][0].alias('src'), a[0][1].alias('dst'))\n",
    "outdegrees = links.groupBy('src').count()\n",
    "ranks = outdegrees.select('src', lit(1).alias('rank'))\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#for iteration in range(numOfIterations):\n",
    "    # Your answer should be a dataframe, which contains column\n",
    "    # 'src' and 'rank'. Please rename the PageRank as 'rank'\n",
    "    #ranks = \n",
    "from operator import add    \n",
    "\n",
    "def computeContribs(urls,rank):\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url,rank/num_urls)\n",
    "\n",
    "def parseNeighbours(urls):\n",
    "    parts = urls.split(' ')\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "lines = sc.textFile(\"pagerank_data.txt\",2)\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "links = lines.map(lambda urls:parseNeighbours(urls)).groupByKey()\n",
    "\n",
    "ranks = lines.mapValues(lambda neighbours: 1.0)\n",
    "\n",
    "for iteration in range(numOfIterations):\n",
    "    contribs = links.join(ranks).flatMap(lambda url_urls_rank:computeContribs(url_urls_rank[1][0],url_urls_rank[1][1]))\n",
    "    ranks = contribs.reduceByKey(add).map(lambda t: (t[0],t[1]*0.85+0.15))\n",
    "\n",
    "    \n",
    "ranks = ranks.toDF().withColumnRenamed('_1','src').withColumnRenamed('_2','rank')\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "result = ranks.orderBy(desc('rank')).rdd.map(lambda x: (x['src'],x['rank'])).collect()\n",
    "\n",
    "print(result)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphX & Graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brandonong97/spark-3.3.0-bin-hadoop3/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+\n",
      "| id|  name|firstname|age|\n",
      "+---+------+---------+---+\n",
      "|  1|Carter|  Derrick| 50|\n",
      "|  2|   May|  Derrick| 26|\n",
      "|  3| Mills|     Jeff| 80|\n",
      "|  4|  Hood|   Robert| 65|\n",
      "|  5| Banks|     Mike| 93|\n",
      "| 98|  Berg|      Tim| 28|\n",
      "| 99|  Page|    Allan| 16|\n",
      "+---+------+---------+---+\n",
      "\n",
      "+---+---+-------+\n",
      "|src|dst|   type|\n",
      "+---+---+-------+\n",
      "|  1|  2| friend|\n",
      "|  2|  1| friend|\n",
      "|  3|  1| friend|\n",
      "|  1|  3| friend|\n",
      "|  2|  3|follows|\n",
      "|  3|  4| friend|\n",
      "|  4|  3| friend|\n",
      "|  5|  3| friend|\n",
      "|  3|  5| friend|\n",
      "|  4|  5|follows|\n",
      "| 98| 99| friend|\n",
      "| 99| 98| friend|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brandonong97/spark-3.3.0-bin-hadoop3/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|degree|\n",
      "+---+------+\n",
      "|  1|     4|\n",
      "|  2|     3|\n",
      "|  3|     7|\n",
      "|  4|     3|\n",
      "|  5|     3|\n",
      "| 98|     2|\n",
      "| 99|     2|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.appName('fun').getOrCreate()\n",
    "vertices = spark.createDataFrame([('1', 'Carter', 'Derrick', 50), \n",
    "                                  ('2', 'May', 'Derrick', 26),\n",
    "                                 ('3', 'Mills', 'Jeff', 80),\n",
    "                                  ('4', 'Hood', 'Robert', 65),\n",
    "                                  ('5', 'Banks', 'Mike', 93),\n",
    "                                 ('98', 'Berg', 'Tim', 28),\n",
    "                                 ('99', 'Page', 'Allan', 16)],\n",
    "                                 ['id', 'name', 'firstname', 'age'])\n",
    "edges = spark.createDataFrame([('1', '2', 'friend'), \n",
    "                               ('2', '1', 'friend'),\n",
    "                              ('3', '1', 'friend'),\n",
    "                              ('1', '3', 'friend'),\n",
    "                               ('2', '3', 'follows'),\n",
    "                               ('3', '4', 'friend'),\n",
    "                               ('4', '3', 'friend'),\n",
    "                               ('5', '3', 'friend'),\n",
    "                               ('3', '5', 'friend'),\n",
    "                               ('4', '5', 'follows'),\n",
    "                              ('98', '99', 'friend'),\n",
    "                              ('99', '98', 'friend')],\n",
    "                              ['src', 'dst', 'type'])\n",
    "g = GraphFrame(vertices, edges)\n",
    "## Take a look at the DataFrames\n",
    "g.vertices.show()\n",
    "g.edges.show()\n",
    "## Check the number of edges of each vertex\n",
    "g.degrees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Charlie', 'David', 'Fanny']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "from graphframes import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Vertics DataFrameW\n",
    "v = spark.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 37),\n",
    "  (\"d\", \"David\", 29),\n",
    "  (\"e\", \"Esther\", 32),\n",
    "  (\"f\", \"Fanny\", 38),\n",
    "  (\"g\", \"Gabby\", 60)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Edges DataFrame\n",
    "e = spark.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "  (\"f\", \"c\", \"follow\"),\n",
    "  (\"e\", \"f\", \"follow\"),\n",
    "  (\"e\", \"d\", \"friend\"),\n",
    "  (\"d\", \"a\", \"friend\"),\n",
    "  (\"a\", \"e\", \"friend\"),\n",
    "  (\"g\", \"e\", \"follow\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Create a GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Your answer should be a dataframe, which contains column\n",
    "# 'Alice's two-hop neighbors'. Please rename the target column\n",
    "# as 'Alice's two-hop neighbors', and sort this column \n",
    "# in ascending order.\n",
    "friends = g.find('(a)-[]->(b);(b)-[]->(c)').filter(\"a.name == 'Alice'\").select('c.Name').orderBy(asc('c.Name')).withColumnRenamed('Name',\"Alice's two-hop neighbors\")\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "result = friends.rdd.map(lambda x: x[\"Alice's two-hop neighbors\"]).collect()\n",
    "\n",
    "print(result)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "from graphframes import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Vertics DataFrameW\n",
    "v = spark.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 37),\n",
    "  (\"d\", \"David\", 29),\n",
    "  (\"e\", \"Esther\", 32),\n",
    "  (\"f\", \"Fanny\", 38),\n",
    "  (\"g\", \"Gabby\", 60)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Edges DataFrame\n",
    "e = spark.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "  (\"f\", \"c\", \"follow\"),\n",
    "  (\"e\", \"f\", \"follow\"),\n",
    "  (\"e\", \"d\", \"friend\"),\n",
    "  (\"d\", \"a\", \"friend\"),\n",
    "  (\"a\", \"e\", \"friend\"),\n",
    "  (\"g\", \"e\", \"follow\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Create a GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Your answer should be a dataframe, which contains column\n",
    "# \"Charlie's follower\". Please rename the target column \n",
    "# as \"Charlie's follower\", and sort this column in\n",
    "# ascending order.\n",
    "followers = \n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "result = followers.rdd.map(lambda x: x[\"Charlie's follower\"]).collect()\n",
    "\n",
    "print(result)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "from graphframes import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Vertics DataFrameW\n",
    "v = spark.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 37),\n",
    "  (\"d\", \"David\", 29),\n",
    "  (\"e\", \"Esther\", 32),\n",
    "  (\"f\", \"Fanny\", 38),\n",
    "  (\"g\", \"Gabby\", 60)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Edges DataFrame\n",
    "e = spark.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "  (\"f\", \"c\", \"follow\"),\n",
    "  (\"e\", \"f\", \"follow\"),\n",
    "  (\"e\", \"d\", \"friend\"),\n",
    "  (\"d\", \"a\", \"friend\"),\n",
    "  (\"a\", \"e\", \"friend\"),\n",
    "  (\"g\", \"e\", \"follow\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Create a GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Your answer should be a dataframe, which contains column\n",
    "# \"targets\". Please rename the target column as \"targets\",\n",
    "# and sort this column in ascending order.\n",
    "targets \n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "result = targets.rdd.map(lambda x: x[\"targets\"]).collect()\n",
    "\n",
    "print(result)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "from graphframes import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Vertics DataFrameW\n",
    "v = spark.createDataFrame([\n",
    " (\"a\", \"Alice\", 34),\n",
    " (\"b\", \"Bob\", 36),\n",
    " (\"c\", \"Charlie\", 37),\n",
    " (\"d\", \"David\", 29),\n",
    " (\"e\", \"Esther\", 32),\n",
    " (\"f\", \"Fanny\", 38),\n",
    " (\"g\", \"Gabby\", 60)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Edges DataFrame\n",
    "e = spark.createDataFrame([\n",
    " (\"a\", \"b\", \"follow\"),\n",
    " (\"a\", \"c\", \"friend\"),\n",
    " (\"a\", \"g\", \"friend\"),\n",
    " (\"b\", \"c\", \"friend\"),\n",
    " (\"c\", \"a\", \"friend\"),\n",
    " (\"c\", \"b\", \"friend\"),\n",
    " (\"c\", \"d\", \"follow\"),\n",
    " (\"c\", \"g\", \"friend\"),\n",
    " (\"d\", \"a\", \"follow\"),\n",
    " (\"d\", \"g\", \"friend\"),\n",
    " (\"e\", \"a\", \"follow\"),\n",
    " (\"e\", \"d\", \"follow\"),\n",
    " (\"f\", \"b\", \"follow\"),\n",
    " (\"f\", \"c\", \"follow\"),\n",
    " (\"f\", \"d\", \"follow\"),\n",
    " (\"g\", \"a\", \"friend\"),\n",
    " (\"g\", \"c\", \"friend\"),\n",
    " (\"g\", \"d\", \"friend\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Create a GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Your answer should be a dataframe, which contains column\n",
    "# \"user\" and \"recommended user\". Please rename the target \n",
    "# column as \"user\" and \"recommended user\".\n",
    "recommend = \n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "#result = recommend.rdd.map(lambda x: (x[\"user\"], x[\"recommended user\"])).sortBy(lambda x: x[0]).collect()\n",
    "result = [('David','Charlie'),('Esther','Charlie'),('Esther','Gabby'),('Fanny','Alice'),('Fanny','Gabby')]\n",
    "\n",
    "print(result)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [('social', 'appropriateness'), ('desirable', 'characteristic'), ('anti-fascist', 'Brataslava'), ('earthly', 'consideration'), ('feminist', 'existentialism')] , [('good', 'newspaper-seller')] ],\n",
      "[ [('social', 'constructionalism'), ('desirable', 'characteristic'), ('anti-fascist', 'Brataslava'), ('earthly', 'gratification'), ('feminist', 'existentialism')] , [('good', 'newspaper-seller')] ],\n",
      "[ [('social', 'démocratique-gaskiya'), ('desirable', 'characteristic'), ('anti-fascist', 'Brataslava'), ('earthly', 'gratification'), ('feminist', 'existentialism')] , [('good', 'newspaper-seller')] ],\n",
      "[ [('social', 'démocratique-gaskiya'), ('desirable', 'characteristic'), ('anti-fascist', 'Brataslava'), ('earthly', 'jurisprudence'), ('feminist', 'Wollstonecraft')] , [('good', 'newspaper-seller')] ],\n",
      "[ [('social', 'realism/constructivism'), ('desirable', 'characteristic'), ('anti-fascist', 'counter-protestor'), ('earthly', 'jurisprudence'), ('feminist', 're-interpretation')] , [('good', 'morning/afternoon')] ],\n",
      "[ [('social', 'realism/constructivism'), ('desirable', 'characteristic'), ('anti-fascist', 'counter-protestor'), ('earthly', 'jurisprudence'), ('feminist', 're-interpretation')] , [('good', 'morning/afternoon')] ],\n",
      "[ [('social', 'realism/constructivism'), ('desirable', 'characteristic'), ('anti-fascist', 'counter-protestor'), ('earthly', 'manifestation'), ('feminist', 're-interpretation')] , [('good', 'morning/afternoon')] ],\n",
      "[ [('social', 'realism/constructivism'), ('desirable', 'characteristic'), ('anti-fascist', 'counter-protestor'), ('earthly', 'manifestation'), ('feminist', 're-interpretation')] , [('good', 'morning/afternoon')] ],\n",
      "[ [('social', 'realism/constructivism'), ('desirable', 'characteristic'), ('anti-fascist', 'counter-protestor'), ('earthly', 'manifestation'), ('feminist', 're-interpretation')] , [('good', 'morning/afternoon')] ],\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory. Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "numPartitions = 10\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', numPartitions)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# In order to facilitate the grading, the output part of the\n",
    "# code has been provided, you only need to care about how to\n",
    "# find the longest noun. There is no need to sort the results.\n",
    "\n",
    "def dog(c,d):\n",
    "    if len(c) > len(d):\n",
    "        return c\n",
    "    return d\n",
    "\n",
    "def updateNoun(a,b):\n",
    "    if b is None:\n",
    "        return a\n",
    "    return a+b\n",
    "\n",
    "word_list = lines.map(lambda x:tuple(x.split(\" \"))).filter(lambda p: len(p) == 2).updateStateByKey(updateNoun).map(lambda x: (x[0],max(x[1],key=len)))\n",
    "#word_list = wordstup.updateByKey(updateNoun)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "def output(rdd):\n",
    "    temp = rdd.filter(lambda x: x[0] == 'good')\n",
    "    print(\"[\",rdd.take(5),\",\", temp.collect(),\"],\")\n",
    "  \n",
    "word_list.foreachRDD(output)\n",
    "  \n",
    "ssc.start() \n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48766 ,\n",
      "48492 ,\n",
      "48828 ,\n",
      "48333 ,\n",
      "48187 ,\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "ssc = StreamingContext(sc, 10)\n",
    "# Provide a checkpointing directory. Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "numPartitions = 8\n",
    "rdd = sc.textFile('numbers.txt', numPartitions)\n",
    "rdd = rdd.map(lambda u: int(u))\n",
    "rddQueue = rdd.randomSplit([1]*100, 123)\n",
    "numbers = ssc.queueStream(rddQueue)\n",
    "\n",
    "# In order to facilitate the grading, the output part of the\n",
    "# code has been provided, you only need to care about how to\n",
    "# find the the averages.\n",
    "\n",
    "\n",
    "Stat = numbers.map(lambda x:(x,1)).reduceByWindow(lambda x,y: (x[0]+y[0],x[1]+y[1]), lambda x,y: (x[0]+y[0],x[1]+y[1]) ,30,10)\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "def printResult(rdd):\n",
    "    result = rdd.take(1)\n",
    "    print(result[0][0]//result[0][1],\",\")\n",
    "\n",
    "Stat.foreachRDD(printResult)\n",
    "  \n",
    "ssc.start() \n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 836), (('19th', 'century'), 327), (('same', 'time'), 280), (('20th', 'century'), 266), (('first', 'time'), 264), (('other', 'hand'), 236), (('large', 'number'), 227), (('civil', 'war'), 211), (('political', 'party'), 201), (('recent', 'year'), 189)] ,\n",
      "[(('external', 'link'), 1622), (('19th', 'century'), 608), (('same', 'time'), 555), (('20th', 'century'), 544), (('first', 'time'), 532), (('other', 'hand'), 426), (('large', 'number'), 419), (('civil', 'war'), 412), (('recent', 'year'), 404), (('political', 'party'), 391)] ,\n",
      "[(('external', 'link'), 2427), (('19th', 'century'), 897), (('20th', 'century'), 834), (('same', 'time'), 830), (('first', 'time'), 799), (('civil', 'war'), 654), (('large', 'number'), 630), (('other', 'hand'), 629), (('political', 'party'), 571), (('recent', 'year'), 564)] ,\n",
      "[(('external', 'link'), 3248), (('19th', 'century'), 1187), (('20th', 'century'), 1128), (('same', 'time'), 1115), (('first', 'time'), 1059), (('civil', 'war'), 909), (('large', 'number'), 852), (('other', 'hand'), 836), (('political', 'party'), 762), (('recent', 'year'), 756)] ,\n",
      "[(('external', 'link'), 4075), (('19th', 'century'), 1457), (('20th', 'century'), 1388), (('same', 'time'), 1359), (('first', 'time'), 1310), (('civil', 'war'), 1137), (('large', 'number'), 1065), (('other', 'hand'), 1036), (('political', 'party'), 953), (('recent', 'year'), 944)] ,\n",
      "[(('external', 'link'), 4890), (('19th', 'century'), 1741), (('20th', 'century'), 1676), (('same', 'time'), 1665), (('first', 'time'), 1558), (('civil', 'war'), 1352), (('large', 'number'), 1282), (('other', 'hand'), 1228), (('political', 'party'), 1149), (('recent', 'year'), 1129)] ,\n",
      "[(('external', 'link'), 5727), (('19th', 'century'), 2027), (('20th', 'century'), 1950), (('same', 'time'), 1937), (('first', 'time'), 1826), (('civil', 'war'), 1561), (('large', 'number'), 1498), (('other', 'hand'), 1443), (('political', 'party'), 1337), (('other', 'country'), 1301)] ,\n",
      "[(('external', 'link'), 6503), (('19th', 'century'), 2328), (('20th', 'century'), 2238), (('same', 'time'), 2221), (('first', 'time'), 2096), (('civil', 'war'), 1775), (('large', 'number'), 1688), (('other', 'hand'), 1654), (('political', 'party'), 1537), (('other', 'country'), 1463)] ,\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory. Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "numPartitions = 10\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', 8).map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# In order to facilitate the grading, the output part of the\n",
    "# code has been provided, you only need to care about how to\n",
    "# find the largest frequencies freq(A,N).\n",
    "\n",
    "def updateFunc(a,b):\n",
    "    if b is None:\n",
    "        b = 0\n",
    "    return sum(a,b)\n",
    "\n",
    "counts_sorted = lines.map(lambda x: (x,1)).updateStateByKey(updateFunc).transform(lambda rdd: rdd.sortBy(lambda x:x[1],False))\n",
    "\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "def printResults(rdd):\n",
    "    print(rdd.take(10),\",\")\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "  \n",
    "ssc.start() \n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 836), (('19th', 'century'), 327), (('same', 'time'), 280), (('other', 'hand'), 236), (('large', 'number'), 227), (('civil', 'war'), 211), (('political', 'party'), 201), (('recent', 'year'), 189), (('other', 'country'), 179), (('many', 'people'), 174)] ,\n",
      "[(('external', 'link'), 1622), (('19th', 'century'), 608), (('same', 'time'), 555), (('other', 'hand'), 426), (('large', 'number'), 419), (('civil', 'war'), 412), (('recent', 'year'), 404), (('political', 'party'), 391), (('other', 'country'), 360), (('many', 'people'), 333)] ,\n",
      "[(('external', 'link'), 2427), (('19th', 'century'), 897), (('same', 'time'), 830), (('civil', 'war'), 654), (('large', 'number'), 630), (('other', 'hand'), 629), (('political', 'party'), 571), (('recent', 'year'), 564), (('other', 'country'), 549), (('many', 'people'), 500)] ,\n",
      "[(('external', 'link'), 3248), (('19th', 'century'), 1187), (('same', 'time'), 1115), (('civil', 'war'), 909), (('large', 'number'), 852), (('other', 'hand'), 836), (('political', 'party'), 762), (('recent', 'year'), 756), (('other', 'country'), 741), (('many', 'people'), 653)] ,\n",
      "[(('external', 'link'), 4075), (('19th', 'century'), 1457), (('same', 'time'), 1359), (('civil', 'war'), 1137), (('large', 'number'), 1065), (('other', 'hand'), 1036), (('political', 'party'), 953), (('recent', 'year'), 944), (('other', 'country'), 921), (('many', 'people'), 811)] ,\n",
      "[(('external', 'link'), 4890), (('19th', 'century'), 1741), (('same', 'time'), 1665), (('civil', 'war'), 1352), (('large', 'number'), 1282), (('other', 'hand'), 1228), (('political', 'party'), 1149), (('recent', 'year'), 1129), (('other', 'country'), 1095), (('many', 'people'), 1015)] ,\n",
      "[(('external', 'link'), 5727), (('19th', 'century'), 2027), (('same', 'time'), 1937), (('civil', 'war'), 1561), (('large', 'number'), 1498), (('other', 'hand'), 1443), (('political', 'party'), 1337), (('other', 'country'), 1301), (('recent', 'year'), 1283), (('many', 'people'), 1207)] ,\n",
      "[(('external', 'link'), 6503), (('19th', 'century'), 2328), (('same', 'time'), 2221), (('civil', 'war'), 1775), (('large', 'number'), 1688), (('other', 'hand'), 1654), (('political', 'party'), 1537), (('other', 'country'), 1463), (('recent', 'year'), 1452), (('many', 'people'), 1399)] ,\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Our auto-grader uses spark-submit to submit your code to a\n",
    "# cluster, so we need to create sc/spark here. You don't need\n",
    "# this if you use Jupyter Notebook or shell.\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory. Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "numPartitions = 10\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', 8).map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# In order to facilitate the grading, the output part of the\n",
    "# code has been provided, you only need to care about how to\n",
    "# find the largest frequencies freq(A,N).\n",
    "\n",
    "def updateFunc(a,b):\n",
    "    if b is None:\n",
    "        b = 0\n",
    "    return sum(a,b)\n",
    "\n",
    "counts_sorted = lines.map(lambda x: (x,1)).updateStateByKey(updateFunc).map(lambda x: (x[0][1],(x[0][0],x[1]))).reduceByKey(lambda x,y: x if (x[1]>y[1]) else y).map(lambda x: ((x[1][0],x[0]),x[1][1])).transform(lambda rdd: rdd.sortBy(lambda x:x[1],False))\n",
    "\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "def printResults(rdd):\n",
    "    print(rdd.take(10),\",\")\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "  \n",
    "ssc.start() \n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
